{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_decent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ANNASBlackHat/Gradient-Decent/blob/master/gradient_decent.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "pSVORx6QiXb1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7111781c-f445-49af-e5e7-3f0bf9847db3"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "parent_folder = '/content/drive/My Drive/Colab Notebooks/Gradient Decent/'\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vt8dtWrpiMKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN4ox5FoiQdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "points = genfromtxt(parent_folder + 'data.csv', delimiter=',')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCzTiBgPjb56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#y = mx + b (slope formula)\n",
        "initial_b = 0\n",
        "initial_m = 0\n",
        "num_iterations = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VexIokLhMinX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/ANNASBlackHat/Gradient-Decent/master/linear_regression_error.png =250x)"
      ]
    },
    {
      "metadata": {
        "id": "J3YWGpp2m55O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_error_for_given_points(b, m , points):\n",
        "  totalError = 0\n",
        "  for i in range(0, len(points)):\n",
        "    x = points[i, 0]\n",
        "    y = points[i, 1]\n",
        "    totalError += (y - (m * x + b)) **2\n",
        "  return totalError / float(len(points))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ukZjup7LYTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://raw.githubusercontent.com/ANNASBlackHat/Gradient-Decent/master/linear%20regression%20-%20gradient%20decent.png =250x)"
      ]
    },
    {
      "metadata": {
        "id": "ZZ1cIt_2mtvV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def step_gradient(b_current, m_current, points, learningRate):\n",
        "  #gradient decent\n",
        "  b_gradient = 0\n",
        "  m_gradient = 0\n",
        "  N = float(len(points))\n",
        "  for i in range(0, len(points)):\n",
        "    x = points[i, 0]\n",
        "    y = points[i, 1] \n",
        "    b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
        "    m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
        "  new_b = b_current - (learningRate * b_gradient)\n",
        "  new_m = m_current - (learningRate * m_gradient)\n",
        "  return [new_b, new_m]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FA4vzgymNUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations) :\n",
        "  b = starting_b\n",
        "  m = starting_m\n",
        "  \n",
        "  for i in range(num_iterations):\n",
        "    b,m = step_gradient(b, m, array(points), learning_rate)\n",
        "  return [b,m]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bM3LzgBAlx-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ecef87d8-88cd-4542-9ff7-a3cbae3bb8d5"
      },
      "cell_type": "code",
      "source": [
        "print \"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_given_points(initial_b, initial_m, points))\n",
        "print \"Running...\"\n",
        "[b,m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "print \"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_given_points(b, m, points))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting gradient descent at b = 0, m = 0, error = 5565.10783448\n",
            "Running...\n",
            "After 1000 iterations b = 0.0889365199374, m = 1.47774408519, error = 112.614810116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-thZmvpLl-QQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}